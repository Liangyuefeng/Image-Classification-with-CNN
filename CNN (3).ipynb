{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IdnsT8VYCSYF"
      },
      "source": [
        "## COMP5623 Coursework on Image Classification with Convolutional Neural Networks \n",
        "\n",
        "Starter code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eKS20wjsCSYI",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from  torch.utils.data import Dataset\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from skimage import io, transform\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import timeit\n",
        "import os\n",
        "import math\n",
        "import cv2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IpRGGlofCSYM"
      },
      "source": [
        "### Part I\n",
        "\n",
        "The first part of the assignment is to build a CNN and train it on a subset of the ImageNet dataset. We will first create a dataframe with all the references to the images and their labels.\n",
        "\n",
        "To download the images into your work environment, clone into a git respository containing the images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EqhQboSCCSYN",
        "outputId": "56a42530-a5b5-422f-e10e-977855a67d95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "! git clone https://github.com/MohammedAlghamdi/imagenet10.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'imagenet10'...\n",
            "remote: Enumerating objects: 10019, done.\u001b[K\n",
            "remote: Total 10019 (delta 0), reused 0 (delta 0), pack-reused 10019\u001b[K\n",
            "Receiving objects: 100% (10019/10019), 966.71 MiB | 55.32 MiB/s, done.\n",
            "Resolving deltas: 100% (2/2), done.\n",
            "Checking out files: 100% (10002/10002), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8edZnYShg7M1"
      },
      "source": [
        "Check that the repository is there:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i2RIZGHaCVAC",
        "outputId": "6c7d4d9d-0f8d-4aa8-f913-a6ea00f6718c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "! ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "imagenet10  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "47tv-wIQCTcS",
        "colab": {}
      },
      "source": [
        "root_dir = \"imagenet10/train_set/\"\n",
        "class_names = [\n",
        "  \"baboon\",\n",
        "  \"banana\",\n",
        "  \"canoe\",\n",
        "  \"cat\",\n",
        "  \"desk\",\n",
        "  \"drill\",\n",
        "  \"dumbbell\",\n",
        "  \"football\",\n",
        "  \"mug\",\n",
        "  \"orange\",\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L6oMdWl5CSYR"
      },
      "source": [
        "A helper function for reading in images and assigning labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tQfcD3jyCSYT",
        "colab": {}
      },
      "source": [
        "def get_meta(root_dir, dirs):\n",
        "    \"\"\" Fetches the meta data for all the images and assigns labels.\n",
        "    \"\"\"\n",
        "    paths, classes = [], []\n",
        "    for i, dir_ in enumerate(dirs):\n",
        "        for entry in os.scandir(root_dir + dir_):\n",
        "            if (entry.is_file()):\n",
        "                paths.append(entry.path)\n",
        "                classes.append(i)\n",
        "                \n",
        "    return paths, classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0J967KW0CSYX"
      },
      "source": [
        "Now we create a dataframe using all the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8gDPs1NCCSYY",
        "colab": {}
      },
      "source": [
        "# Benign images we will assign class 0, and malignant as 1\n",
        "paths, classes = get_meta(root_dir, class_names)\n",
        "\n",
        "data = {\n",
        "    'path': paths,\n",
        "    'class': classes\n",
        "}\n",
        "\n",
        "data_df = pd.DataFrame(data, columns=['path', 'class'])\n",
        "data_df = data_df.sample(frac=1).reset_index(drop=True) # Shuffles the data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jRUDj3WihItY"
      },
      "source": [
        "View some sample data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mK2GPzfVCSYc",
        "outputId": "ed42d0d6-a606-4879-942d-7b91870687e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "print(\"Found\", len(data_df), \"images.\")\n",
        "data_df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 9000 images.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>imagenet10/train_set/drill/n03239726_1686.JPEG</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>imagenet10/train_set/football/n04254680_2170.JPEG</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>imagenet10/train_set/drill/n03239726_19589.JPEG</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>imagenet10/train_set/desk/n03179701_46928.JPEG</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>imagenet10/train_set/banana/n07753592_7578.JPEG</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                path  class\n",
              "0     imagenet10/train_set/drill/n03239726_1686.JPEG      5\n",
              "1  imagenet10/train_set/football/n04254680_2170.JPEG      7\n",
              "2    imagenet10/train_set/drill/n03239726_19589.JPEG      5\n",
              "3     imagenet10/train_set/desk/n03179701_46928.JPEG      4\n",
              "4    imagenet10/train_set/banana/n07753592_7578.JPEG      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ai3uvziWCSYh"
      },
      "source": [
        "Now we will create the Dataset class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EyUb-rzQCSYi",
        "colab": {}
      },
      "source": [
        "class ImageNet10(Dataset):\n",
        "    \"\"\" ImageNet10 dataset. \"\"\"\n",
        "\n",
        "    def __init__(self, df, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_dir (string): Directory with all the images\n",
        "            df (DataFrame object): Dataframe containing the images, paths and classes\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Load image from path and get label\n",
        "        x = Image.open(self.df['path'][index])\n",
        "        try:\n",
        "          x = x.convert('RGB') # To deal with some grayscale images in the data\n",
        "        except:\n",
        "          pass\n",
        "        y = torch.tensor(int(self.df['class'][index]))\n",
        "\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "\n",
        "        return x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vqtRjBozCSYk"
      },
      "source": [
        "Compute what we should normalise the dataset to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pPqfMPuZCSYl",
        "colab": {}
      },
      "source": [
        "def compute_img_mean_std(image_paths):\n",
        "    \"\"\"\n",
        "        Author: @xinruizhuang. Computing the mean and std of three channel on the whole dataset,\n",
        "        first we should normalize the image from 0-255 to 0-1\n",
        "    \"\"\"\n",
        "\n",
        "    img_h, img_w = 224, 224\n",
        "    imgs = []\n",
        "    means, stdevs = [], []\n",
        "\n",
        "    for i in tqdm(range(len(image_paths))):\n",
        "        img = cv2.imread(image_paths[i])\n",
        "        img = cv2.resize(img, (img_h, img_w))\n",
        "        imgs.append(img)\n",
        "\n",
        "    imgs = np.stack(imgs, axis=3)\n",
        "    print(imgs.shape)\n",
        "\n",
        "    imgs = imgs.astype(np.float32) / 255.\n",
        "\n",
        "    for i in range(3):\n",
        "        pixels = imgs[:, :, i, :].ravel()  # resize to one row\n",
        "        means.append(np.mean(pixels))\n",
        "        stdevs.append(np.std(pixels))\n",
        "\n",
        "    means.reverse()  # BGR --> RGB\n",
        "    stdevs.reverse()\n",
        "\n",
        "    print(\"normMean = {}\".format(means))\n",
        "    print(\"normStd = {}\".format(stdevs))\n",
        "    return means, stdevs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-_1KSGUe4sEW",
        "outputId": "a6a16191-5e1b-4fd4-8f4f-ad4d7039b7e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "#norm_mean, norm_std = compute_img_mean_std(paths)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 19%|█▉        | 1717/9000 [00:08<00:34, 212.09it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-f7546331b166>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnorm_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_img_mean_std\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-3c9260f7136d>\u001b[0m in \u001b[0;36mcompute_img_mean_std\u001b[0;34m(image_paths)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LLA1yvk0MLB7",
        "colab": {}
      },
      "source": [
        "norm_mean = [0.5228344, 0.47988218, 0.40605018]\n",
        "norm_std = [0.29770824, 0.28884, 0.31178245]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RqrG3FCUCSYo"
      },
      "source": [
        "Now let's create the transforms to normalise and turn our data into tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OVeZpgM-CSYq",
        "colab": {}
      },
      "source": [
        "data_transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(256),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(norm_mean, norm_std),\n",
        "    ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cAPzCfr2CSYt"
      },
      "source": [
        "Let's split the data into train and test sets and instantiate our new ISIC_Dataset objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "POCexxXjCSYu",
        "colab": {}
      },
      "source": [
        "train_split = 0.70 # Defines the ratio of train/valid/test data.\n",
        "valid_split = 0.10\n",
        "\n",
        "train_size = int(len(data_df)*train_split)\n",
        "valid_size = int(len(data_df)*valid_split)\n",
        "\n",
        "ins_dataset_train = ImageNet10(\n",
        "    df=data_df[:train_size],\n",
        "    transform=data_transform,\n",
        ")\n",
        "\n",
        "ins_dataset_valid = ImageNet10(\n",
        "    df=data_df[train_size:(train_size + valid_size)].reset_index(drop=True),\n",
        "    transform=data_transform,\n",
        ")\n",
        "\n",
        "ins_dataset_test = ImageNet10(\n",
        "    df=data_df[(train_size + valid_size):].reset_index(drop=True),\n",
        "    transform=data_transform,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PzVfNzvmhTGJ"
      },
      "source": [
        "You will need to create DataLoaders for the datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8DeAdM2VMFGI",
        "colab": {}
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    ins_dataset_train,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    ins_dataset_test,\n",
        "    batch_size=32, # Forward pass only so batch size can be larger\n",
        "    shuffle=False,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    ins_dataset_valid, \n",
        "    batch_size=16,\n",
        "    shuffle=True,  \n",
        "    num_workers=2)\n",
        "\n",
        "classes=np.arange(0,10)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "X00vuzPWhY5h"
      },
      "source": [
        "A framework for the ConvNet model, missing all layers except the final fully-connected layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cZhS9q9twEXH",
        "outputId": "96be6881-a580-4f09-d5cc-a35e18b18b73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "source": [
        "# Convolutional neural network\n",
        "class ConvNet(nn.Module):\n",
        "    \n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ConvNet, self).__init__()\n",
        "  \n",
        "        # Add network layers here\n",
        "        self.conv1=torch.nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3,out_channels=16,kernel_size=5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
        "            torch.nn.Dropout(0.6)\n",
        "        )\n",
        "\n",
        "\n",
        "        self.conv2=torch.nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16,out_channels=24,kernel_size=5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
        "            torch.nn.Dropout(0.6)\n",
        "        )\n",
        "\n",
        "        self.conv3=torch.nn.Sequential(\n",
        "           nn.Conv2d(in_channels=24,out_channels=32,kernel_size=5),\n",
        "           nn.ReLU(),\n",
        "           nn.MaxPool2d(kernel_size=2,stride=2),\n",
        "           torch.nn.Dropout(0.5)\n",
        "        )\n",
        "\n",
        "        self.conv4=torch.nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32,out_channels=64,kernel_size=5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
        "            torch.nn.Dropout(0.6)\n",
        "        )\n",
        "      \n",
        "        \n",
        "        self.fc1 = nn.Linear(9216,512)   # 8*8*128\n",
        "        self.fc2 = nn.Linear(512,num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Complete the graph\n",
        "        out = self.conv1(x)\n",
        "        out = self.conv2(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.conv4(out)\n",
        "        # out = x.reshape(x.size(0), -1) # TODO what does this do? Why do we need it?\n",
        "        out = out.view(out.size(0),-1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        return out\n",
        "      \n",
        "net = ConvNet()\n",
        "print(net)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ConvNet(\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Dropout(p=0.6, inplace=False)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(16, 24, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Dropout(p=0.6, inplace=False)\n",
            "  )\n",
            "  (conv3): Sequential(\n",
            "    (0): Conv2d(24, 32, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "  (conv4): Sequential(\n",
            "    (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Dropout(p=0.6, inplace=False)\n",
            "  )\n",
            "  (fc1): Linear(in_features=9216, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gjplp2DnwbWT",
        "outputId": "c9ba17af-816d-44e8-a5a5-d384a297c401",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Device configuration - defaults to CPU unless GPU is available on device\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S6svhXHswh5O",
        "colab": {}
      },
      "source": [
        "model_gpu=ConvNet().to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sUsGg1J43DkT",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim          \n",
        " \n",
        "criterion = nn.CrossEntropyLoss()    \n",
        "optimizer = optim.SGD(model_gpu.parameters(), lr=0.1, momentum=0.9)   #optim模块中的SGD梯度优化方式---随机梯度下降\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Whwsm9OTwu5x",
        "colab": {}
      },
      "source": [
        "t_loss = []\n",
        "v_loss = []\n",
        "epochs = []\n",
        "\"\"\" Trains the model for a given number of epochs on the training set. \"\"\"\n",
        "def train_model_epochs(num_epochs):\n",
        "     for epoch in range(num_epochs):\n",
        "      \n",
        "          train_loss=0.0\n",
        "          for i, data in enumerate(train_loader, 0):\n",
        "               images, labels = data\n",
        "\n",
        "               images = images.to(device)\n",
        "               labels = labels.to(device)\n",
        "\n",
        "               # Zero the parameter gradients means to reset them from\n",
        "               # any previous values. By default, gradients accumulate!\n",
        "               optimizer.zero_grad()\n",
        "\n",
        "               # Passing inputs to the model calls the forward() function of\n",
        "               # the Module class, and the outputs value contains the return value\n",
        "               # of forward()\n",
        "               outputs = model_gpu(images)\n",
        "\n",
        "               # Compute the loss based on the true labels\n",
        "               loss = criterion(outputs, labels)\n",
        "\n",
        "               # Backpropagate the error with respect to the loss\n",
        "               loss.backward()\n",
        "\n",
        "               # Updates the parameters based on current gradients and update rule;\n",
        "               # in this case, defined by SGD()\n",
        "               optimizer.step()\n",
        "\n",
        "               # Print our loss\n",
        "               train_loss += loss.item()\n",
        "               if i % 100 == 99:    # print every 100 mini-batches\n",
        "                   print('Epoch / Batch [%d / %d] - train_Loss: %.3f' %\n",
        "                         (epoch + 1, i + 1, train_loss / 1000))\n",
        "                   t_loss.append(train_loss / 1000)\n",
        "                   train_loss = 0.0\n",
        "          \n",
        "\n",
        "    \n",
        "          total_val_loss = 0\n",
        "          for j,val_data in enumerate(val_loader,0):\n",
        "              # Wrap tensors in Variables\n",
        "              inputs, labels = val_data\n",
        "\n",
        "              inputs = inputs.to(device)\n",
        "              labels = labels.to(device)\n",
        " \n",
        "              # Forward pass\n",
        "              val_outputs = model_gpu(inputs)\n",
        "\n",
        "              val_loss = criterion(val_outputs, labels)\n",
        "             \n",
        "              total_val_loss += val_loss.item()\n",
        "              if j % 10 == 9:                         # print every 100 mini-batches\n",
        "                   print('Epoch / Batch [%d / %d] - Val_Loss: %.3f' %\n",
        "                        (epoch + 1, j + 1, total_val_loss / 100))\n",
        "                   v_loss.append(total_val_loss / 100)\n",
        "                   total_val_loss = 0.0\n",
        "                            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iEgkAcf0-fMM",
        "outputId": "7b519d0b-6881-4c97-f00c-fcc83cec4649",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# train_model_epochs(num_epochs)\n",
        "\n",
        "gpu_train_time = timeit.timeit(\n",
        "    \"train_model_epochs(num_epochs)\",\n",
        "    setup=\"num_epochs=10\",\n",
        "    number=1,\n",
        "    globals=globals(),\n",
        ")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch / Batch [1 / 100] - train_Loss: 0.264\n",
            "Epoch / Batch [1 / 200] - train_Loss: 0.237\n",
            "Epoch / Batch [1 / 300] - train_Loss: 0.234\n",
            "Epoch / Batch [1 / 10] - Val_Loss: 0.240\n",
            "Epoch / Batch [1 / 20] - Val_Loss: 0.234\n",
            "Epoch / Batch [1 / 30] - Val_Loss: 0.238\n",
            "Epoch / Batch [1 / 40] - Val_Loss: 0.234\n",
            "Epoch / Batch [1 / 50] - Val_Loss: 0.237\n",
            "Epoch / Batch [2 / 100] - train_Loss: 0.233\n",
            "Epoch / Batch [2 / 200] - train_Loss: 0.231\n",
            "Epoch / Batch [2 / 300] - train_Loss: 0.233\n",
            "Epoch / Batch [2 / 10] - Val_Loss: 0.231\n",
            "Epoch / Batch [2 / 20] - Val_Loss: 0.232\n",
            "Epoch / Batch [2 / 30] - Val_Loss: 0.229\n",
            "Epoch / Batch [2 / 40] - Val_Loss: 0.231\n",
            "Epoch / Batch [2 / 50] - Val_Loss: 0.230\n",
            "Epoch / Batch [3 / 100] - train_Loss: 0.232\n",
            "Epoch / Batch [3 / 200] - train_Loss: 0.232\n",
            "Epoch / Batch [3 / 300] - train_Loss: 0.232\n",
            "Epoch / Batch [3 / 10] - Val_Loss: 0.234\n",
            "Epoch / Batch [3 / 20] - Val_Loss: 0.231\n",
            "Epoch / Batch [3 / 30] - Val_Loss: 0.230\n",
            "Epoch / Batch [3 / 40] - Val_Loss: 0.233\n",
            "Epoch / Batch [3 / 50] - Val_Loss: 0.232\n",
            "Epoch / Batch [4 / 100] - train_Loss: 0.232\n",
            "Epoch / Batch [4 / 200] - train_Loss: 0.232\n",
            "Epoch / Batch [4 / 300] - train_Loss: 0.232\n",
            "Epoch / Batch [4 / 10] - Val_Loss: 0.233\n",
            "Epoch / Batch [4 / 20] - Val_Loss: 0.232\n",
            "Epoch / Batch [4 / 30] - Val_Loss: 0.230\n",
            "Epoch / Batch [4 / 40] - Val_Loss: 0.231\n",
            "Epoch / Batch [4 / 50] - Val_Loss: 0.230\n",
            "Epoch / Batch [5 / 100] - train_Loss: 0.231\n",
            "Epoch / Batch [5 / 200] - train_Loss: 0.232\n",
            "Epoch / Batch [5 / 300] - train_Loss: 0.232\n",
            "Epoch / Batch [5 / 10] - Val_Loss: 0.232\n",
            "Epoch / Batch [5 / 20] - Val_Loss: 0.235\n",
            "Epoch / Batch [5 / 30] - Val_Loss: 0.233\n",
            "Epoch / Batch [5 / 40] - Val_Loss: 0.234\n",
            "Epoch / Batch [5 / 50] - Val_Loss: 0.234\n",
            "Epoch / Batch [6 / 100] - train_Loss: 0.232\n",
            "Epoch / Batch [6 / 200] - train_Loss: 0.232\n",
            "Epoch / Batch [6 / 300] - train_Loss: 0.232\n",
            "Epoch / Batch [6 / 10] - Val_Loss: 0.232\n",
            "Epoch / Batch [6 / 20] - Val_Loss: 0.233\n",
            "Epoch / Batch [6 / 30] - Val_Loss: 0.232\n",
            "Epoch / Batch [6 / 40] - Val_Loss: 0.232\n",
            "Epoch / Batch [6 / 50] - Val_Loss: 0.231\n",
            "Epoch / Batch [7 / 100] - train_Loss: 0.232\n",
            "Epoch / Batch [7 / 200] - train_Loss: 0.232\n",
            "Epoch / Batch [7 / 300] - train_Loss: 0.232\n",
            "Epoch / Batch [7 / 10] - Val_Loss: 0.231\n",
            "Epoch / Batch [7 / 20] - Val_Loss: 0.231\n",
            "Epoch / Batch [7 / 30] - Val_Loss: 0.231\n",
            "Epoch / Batch [7 / 40] - Val_Loss: 0.232\n",
            "Epoch / Batch [7 / 50] - Val_Loss: 0.231\n",
            "Epoch / Batch [8 / 100] - train_Loss: 0.232\n",
            "Epoch / Batch [8 / 200] - train_Loss: 0.232\n",
            "Epoch / Batch [8 / 300] - train_Loss: 0.232\n",
            "Epoch / Batch [8 / 10] - Val_Loss: 0.232\n",
            "Epoch / Batch [8 / 20] - Val_Loss: 0.232\n",
            "Epoch / Batch [8 / 30] - Val_Loss: 0.232\n",
            "Epoch / Batch [8 / 40] - Val_Loss: 0.231\n",
            "Epoch / Batch [8 / 50] - Val_Loss: 0.231\n",
            "Epoch / Batch [9 / 100] - train_Loss: 0.232\n",
            "Epoch / Batch [9 / 200] - train_Loss: 0.232\n",
            "Epoch / Batch [9 / 300] - train_Loss: 0.231\n",
            "Epoch / Batch [9 / 10] - Val_Loss: 0.233\n",
            "Epoch / Batch [9 / 20] - Val_Loss: 0.232\n",
            "Epoch / Batch [9 / 30] - Val_Loss: 0.231\n",
            "Epoch / Batch [9 / 40] - Val_Loss: 0.233\n",
            "Epoch / Batch [9 / 50] - Val_Loss: 0.234\n",
            "Epoch / Batch [10 / 100] - train_Loss: 0.231\n",
            "Epoch / Batch [10 / 200] - train_Loss: 0.233\n",
            "Epoch / Batch [10 / 300] - train_Loss: 0.231\n",
            "Epoch / Batch [10 / 10] - Val_Loss: 0.230\n",
            "Epoch / Batch [10 / 20] - Val_Loss: 0.232\n",
            "Epoch / Batch [10 / 30] - Val_Loss: 0.231\n",
            "Epoch / Batch [10 / 40] - Val_Loss: 0.229\n",
            "Epoch / Batch [10 / 50] - Val_Loss: 0.233\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NSb7jupIWix",
        "colab_type": "code",
        "outputId": "31326732-2aa6-4f7c-d680-055843666a23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plt.figure('training_loss VS validation_loss')\n",
        "plt.subplot(211)\n",
        "plt.plot(range(len(t_loss)),t_loss,color='blue',label='training_loss')\n",
        "plt.legend()\n",
        "plt.subplot(212)\n",
        "plt.plot(range(len(v_loss)),v_loss,color='green',label='validation_loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cFoplA3A4Unp",
        "outputId": "18cc2172-c98f-42ec-b7aa-b70d9a0a62bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# test model， 测试模型\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Why don't we need gradients? What happens if we do include gradients?\n",
        "with torch.no_grad():\n",
        "\n",
        "    # Iterate over the test set\n",
        "    for data in test_loader:\n",
        "          images, labels = data\n",
        "          \n",
        "          images = images.to(device)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          outputs = model_gpu(images)\n",
        "\n",
        "          # torch.max is an argmax operation\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the test images: %d %%' % (100 * correct / total))\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the test images: 10 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "udX-ibqE5hjA",
        "colab": {}
      },
      "source": [
        "torch.save(model_gpu,'./my_mnist_model.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zGByXUsM5lLE",
        "colab": {}
      },
      "source": [
        "cm = confusion_matrix(labels.cpu(), predicted.cpu()) \n",
        "cm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BLMJ2Zcw5uPl",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "\n",
        "def plot_confusion_matrix(cm,\n",
        "                          classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix very prettily.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "\n",
        "    # Specify the tick marks and axis text\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=90)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    # The data formatting\n",
        "    \n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    \n",
        "    # Print the text of the matrix, adjusting text colour for display\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IAsVt9uT562B",
        "colab": {}
      },
      "source": [
        "plot_confusion_matrix(cm, classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3tBA7zY8W_f0"
      },
      "source": [
        "**第二题，好像是这样做。。。。。**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ut-iZmFnLLpQ",
        "colab": {}
      },
      "source": [
        "# 可视化卷积核在训练前，训练中，训练后  \n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import utils\n",
        "\n",
        "def visTensor(tensor, ch=0, allkernels=True, nrow=6, padding=1): \n",
        "    n,c,w,h = tensor.shape\n",
        "\n",
        "    if allkernels: \n",
        "      tensor = tensor.view(n*c, -1, w, h)\n",
        "    elif c != 3: tensor = tensor[:,ch,:,:].unsqueeze(dim=1)\n",
        "\n",
        "    rows = np.min((tensor.shape[0] // nrow + 1, 64))    \n",
        "    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)\n",
        "    plt.figure( figsize=(nrow,rows) )\n",
        "    plt.imshow(grid.numpy().transpose((1, 2, 0)),cmap=\"gray\")\n",
        "\n",
        "\n",
        "f = model_gpu.conv1[0].weight.cpu().detach().clone()\n",
        "# filter1 = net.conv1[0].weight.data.clone()\n",
        "visTensor(f, ch=0, allkernels=True)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ExAinKlUUs2a"
      },
      "source": [
        "**第三题，可视化原始图像，第一层卷积中的4张图像，（卷积后，激活函数后，maxpooling后，dropout后）。。。。。**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q7DC8DJbSehe",
        "colab": {}
      },
      "source": [
        "for data in test_loader:\n",
        "      images, labels = data\n",
        "\n",
        "print(type(images))\n",
        "\n",
        "images1 = images.numpy()\n",
        "\n",
        "img = np.squeeze(images1)\n",
        "print(img.shape)\n",
        "img = img[0,:,:,:]\n",
        "print(img.shape)\n",
        "pic = np.rollaxis(img,0,3)\n",
        "# print(pic.shape)\n",
        "\n",
        "plt.imshow(pic)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WmmLSWxhYexH",
        "colab": {}
      },
      "source": [
        "#定义用于hook的类\n",
        "class LayerActivations():\n",
        "\t#定义这个变量用于储存结果\n",
        "\tfeatures = None\n",
        "\t#类初始化。当前向传播的时候（即图像数据通过层传输的时候），调用register_forward_hook方法。\n",
        "\t#register_forward_hook方法即为[钩子]，此方法返回一个句柄保存到self.hook\n",
        "\tdef __init__(self,model,layer_num):\n",
        "\t\tself.hook = model[layer_num].register_forward_hook(self.hook_fn)\n",
        "\t#hook函数具体执行的方法，即hook方法\n",
        "\t#register_forward_hook将三个参数传入hook_fn方法内\n",
        "\t#module:允许访问层本身 input:流进层的数据 output:层变换后的流出的数据或激活\n",
        "\tdef hook_fn(self,module,input,output):\n",
        "\t\t#将输出保存到[自己设置的features变量中]\n",
        "\t\tself.features = output.cpu()\n",
        "\t#注销句柄self.hook\n",
        "\tdef remove(self):\n",
        "\t\tself.hook.remove()\n",
        "\n",
        "#定义hook类实例\n",
        "conv_out = LayerActivations(net.conv2,0)  # 提出第一个卷积层的输出 \n",
        "\n",
        "#运行模型\n",
        "output = net(images)\n",
        "#注销函数\n",
        "conv_out.remove()\n",
        "\n",
        "# 在hook class中被保存到了features变量的即为输出，自己定义的\n",
        "activations = conv_out.features\n",
        "#activations 即为层输出\n",
        "\n",
        "#对其进行可视化\n",
        "fig = plt.figure(figsize=(20,50))\n",
        "fig.subplots_adjust(left=0,right=1,bottom=0,top=0.8,hspace=0,wspace=0.2)\n",
        "for i in range(30):\n",
        "   ax = fig.add_subplot(12,5,i+1,xticks=[],yticks=[])\n",
        "   ax.imshow(activations[0][i].detach().numpy())\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}